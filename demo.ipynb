{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "722b997b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "UserWarning: Failed to load custom C++ ops. Running on CPU mode Only!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import open3d as o3d\n",
    "import torch.nn.functional as F\n",
    "import supervision as sv\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from Grounded_Segment_Anything.GroundingDINO.groundingdino.util import box_ops\n",
    "from Grounded_Segment_Anything.GroundingDINO.groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "from Grounded_Segment_Anything.segment_anything.segment_anything import sam_model_registry, SamPredictor, SamAutomaticMaskGenerator\n",
    "\n",
    "\n",
    "CONFIG_PATH = \"./Grounded_Segment_Anything/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
    "CHECKPOINT_PATH = \"./models/groundingdino_swint_ogc.pth\"\n",
    "SAM_CHECKPOINT = \"./models/sam_vit_h_4b8939.pth\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "TEXT_PROMPT = \"ear\"\n",
    "BOX_THRESHOLD = 0.3\n",
    "TEXT_THRESHOLD = 0.25\n",
    "VIEWS_DIR = \"./render_views/bunny/\"\n",
    "OUTPUT_DIR = \"./new/newnewnew/\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ab3ed09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n",
      "Found 5 views to process\n",
      "Processing view_0.png...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SupervisionWarnings: annotate is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n",
      "DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results for view_0.png\n",
      "Processing view_1.png...\n",
      "Error processing view_1.png: xyxy must be a 2D np.ndarray with shape (_, 4), but got shape (2, 1, 4)\n",
      "Processing view_2.png...\n",
      "Error processing view_2.png: xyxy must be a 2D np.ndarray with shape (_, 4), but got shape (2, 1, 4)\n",
      "Processing view_3.png...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SupervisionWarnings: annotate is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 186\u001b[39m\n\u001b[32m    183\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError processing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mview_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     \u001b[43msegment_and_save_views\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSegmentation complete! Check the output directory for results.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 178\u001b[39m, in \u001b[36msegment_and_save_views\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    175\u001b[39m     highlighted_on_original = extract_segmented_object(image_source, binary_mask)\u001b[38;5;66;03m#masked\u001b[39;00m\n\u001b[32m    176\u001b[39m     cv2.imwrite(os.path.join(OUTPUT_DIR, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_masked_overlay.png\u001b[39m\u001b[33m\"\u001b[39m), highlighted_on_original)\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     \u001b[43mauto_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_source\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSaved results for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mview_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 110\u001b[39m, in \u001b[36mauto_mask\u001b[39m\u001b[34m(image_source, base_name)\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mauto_mask\u001b[39m(image_source, base_name):\n\u001b[32m    109\u001b[39m     mask_generator = SamAutomaticMaskGenerator(sam)\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     masks = \u001b[43mmask_generator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_source\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m     \u001b[38;5;66;03m# Pick the largest mask (or you could use CLIP scoring here)\u001b[39;00m\n\u001b[32m    113\u001b[39m     best_mask = \u001b[38;5;28msorted\u001b[39m(masks, key=\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[33m'\u001b[39m\u001b[33marea\u001b[39m\u001b[33m'\u001b[39m], reverse=\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33msegmentation\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/llm_data_vis/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/llm_data_vis/Grounded_Segment_Anything/segment_anything/segment_anything/automatic_mask_generator.py:163\u001b[39m, in \u001b[36mSamAutomaticMaskGenerator.generate\u001b[39m\u001b[34m(self, image)\u001b[39m\n\u001b[32m    138\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    139\u001b[39m \u001b[33;03mGenerates masks for the given image.\u001b[39;00m\n\u001b[32m    140\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    159\u001b[39m \u001b[33;03m         the mask, given in XYWH format.\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    162\u001b[39m \u001b[38;5;66;03m# Generate masks\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m mask_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_masks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[38;5;66;03m# Filter small disconnected regions and holes in masks\u001b[39;00m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.min_mask_region_area > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/llm_data_vis/Grounded_Segment_Anything/segment_anything/segment_anything/automatic_mask_generator.py:206\u001b[39m, in \u001b[36mSamAutomaticMaskGenerator._generate_masks\u001b[39m\u001b[34m(self, image)\u001b[39m\n\u001b[32m    204\u001b[39m data = MaskData()\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m crop_box, layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(crop_boxes, layer_idxs):\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m     crop_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_crop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_box\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    207\u001b[39m     data.cat(crop_data)\n\u001b[32m    209\u001b[39m \u001b[38;5;66;03m# Remove duplicate masks between crops\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/llm_data_vis/Grounded_Segment_Anything/segment_anything/segment_anything/automatic_mask_generator.py:245\u001b[39m, in \u001b[36mSamAutomaticMaskGenerator._process_crop\u001b[39m\u001b[34m(self, image, crop_box, crop_layer_idx, orig_size)\u001b[39m\n\u001b[32m    243\u001b[39m data = MaskData()\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (points,) \u001b[38;5;129;01min\u001b[39;00m batch_iterator(\u001b[38;5;28mself\u001b[39m.points_per_batch, points_for_image):\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m     batch_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcropped_im_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_box\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    246\u001b[39m     data.cat(batch_data)\n\u001b[32m    247\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m batch_data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/llm_data_vis/Grounded_Segment_Anything/segment_anything/segment_anything/automatic_mask_generator.py:300\u001b[39m, in \u001b[36mSamAutomaticMaskGenerator._process_batch\u001b[39m\u001b[34m(self, points, im_size, crop_box, orig_size)\u001b[39m\n\u001b[32m    297\u001b[39m     data.filter(keep_mask)\n\u001b[32m    299\u001b[39m \u001b[38;5;66;03m# Calculate stability score\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m data[\u001b[33m\"\u001b[39m\u001b[33mstability_score\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mcalculate_stability_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmasks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmask_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstability_score_offset\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stability_score_thresh > \u001b[32m0.0\u001b[39m:\n\u001b[32m    304\u001b[39m     keep_mask = data[\u001b[33m\"\u001b[39m\u001b[33mstability_score\u001b[39m\u001b[33m\"\u001b[39m] >= \u001b[38;5;28mself\u001b[39m.stability_score_thresh\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/llm_data_vis/Grounded_Segment_Anything/segment_anything/segment_anything/utils/amg.py:173\u001b[39m, in \u001b[36mcalculate_stability_score\u001b[39m\u001b[34m(masks, mask_threshold, threshold_offset)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;66;03m# One mask is always contained inside the other.\u001b[39;00m\n\u001b[32m    165\u001b[39m \u001b[38;5;66;03m# Save memory by preventing unnecesary cast to torch.int64\u001b[39;00m\n\u001b[32m    166\u001b[39m intersections = (\n\u001b[32m    167\u001b[39m     (masks > (mask_threshold + threshold_offset))\n\u001b[32m    168\u001b[39m     .sum(-\u001b[32m1\u001b[39m, dtype=torch.int16)\n\u001b[32m    169\u001b[39m     .sum(-\u001b[32m1\u001b[39m, dtype=torch.int32)\n\u001b[32m    170\u001b[39m )\n\u001b[32m    171\u001b[39m unions = (\n\u001b[32m    172\u001b[39m     \u001b[43m(\u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask_threshold\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold_offset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mint16\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m     .sum(-\u001b[32m1\u001b[39m, dtype=torch.int32)\n\u001b[32m    175\u001b[39m )\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m intersections / unions\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "groundingdino_model = load_model(CONFIG_PATH, CHECKPOINT_PATH).to(DEVICE)\n",
    "sam = sam_model_registry[\"vit_h\"](checkpoint=SAM_CHECKPOINT).to(DEVICE)\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(DEVICE)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "sam_predictor = SamPredictor(sam)\n",
    "\n",
    "def rerank_boxes_with_clip(image_source, boxes, text_prompt, top_k=1):\n",
    "    \"\"\"\n",
    "    Refine box selection using CLIP image-text similarity.\n",
    "    \"\"\"\n",
    "    H, W, _ = image_source.shape\n",
    "    boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H])\n",
    "    boxes_xyxy = boxes_xyxy.int()\n",
    "\n",
    "    cropped_images = []\n",
    "    for box in boxes_xyxy.view(-1, 4):\n",
    "        x1, y1, x2, y2 = box.tolist()\n",
    "        crop = image_source[y1:y2, x1:x2]\n",
    "        if crop.size == 0:  # avoid invalid crops\n",
    "            continue\n",
    "        crop_pil = Image.fromarray(crop).convert(\"RGB\")\n",
    "        cropped_images.append(crop_pil)\n",
    "\n",
    "    if not cropped_images:\n",
    "        return torch.empty((0, 4), dtype=boxes.dtype).to(boxes.device)\n",
    "\n",
    "    inputs = clip_processor(\n",
    "        text=[text_prompt] * len(cropped_images),\n",
    "        images=cropped_images,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = clip_model(**inputs)\n",
    "        logits_per_image = outputs.logits_per_image  # shape [N, 1]\n",
    "        probs = F.softmax(logits_per_image.squeeze(), dim=0)\n",
    "\n",
    "    # Select top-k indices and remove extra dimensions\n",
    "    top_indices = probs.topk(top_k).indices\n",
    "    filtered_boxes = boxes[top_indices]\n",
    "    if filtered_boxes.ndim == 1:\n",
    "        filtered_boxes = filtered_boxes.unsqueeze(0)\n",
    "\n",
    "    return filtered_boxes\n",
    "\n",
    "\n",
    "def segment(image, sam_model, boxes):\n",
    "  sam_model.set_image(image)\n",
    "  H, W, _ = image.shape\n",
    "  boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H])\n",
    "\n",
    "  transformed_boxes = sam_model.transform.apply_boxes_torch(boxes_xyxy.to(DEVICE), image.shape[:2])\n",
    "  masks, _, _ = sam_model.predict_torch(\n",
    "      point_coords = None,\n",
    "      point_labels = None,\n",
    "      boxes = transformed_boxes,\n",
    "      multimask_output = False,\n",
    "      )\n",
    "  return masks.cpu()\n",
    "  \n",
    "\n",
    "def draw_mask(mask, image, random_color=True):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.8])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    \n",
    "    annotated_frame_pil = Image.fromarray(image).convert(\"RGBA\")\n",
    "    mask_image_pil = Image.fromarray((mask_image.cpu().numpy() * 255).astype(np.uint8)).convert(\"RGBA\")\n",
    "\n",
    "    return np.array(Image.alpha_composite(annotated_frame_pil, mask_image_pil))\n",
    "\n",
    "def extract_segmented_object(image, mask):\n",
    "    \"\"\"Extracts only the masked object from the image (black background).\"\"\"\n",
    "    binary_mask = (mask > 0).astype(np.uint8)\n",
    "\n",
    "    # Apply the mask to each channel\n",
    "    segmented = cv2.bitwise_and(image, image, mask=binary_mask)\n",
    "\n",
    "    return segmented\n",
    "\n",
    "def box_to_pixel(box, image_shape):\n",
    "    h, w = image_shape[:2]\n",
    "    cx, cy, bw, bh = box\n",
    "    x1 = int((cx - bw / 2) * w)\n",
    "    y1 = int((cy - bh / 2) * h)\n",
    "    x2 = int((cx + bw / 2) * w)\n",
    "    y2 = int((cy + bh / 2) * h)\n",
    "    return np.array([x1, y1, x2, y2])\n",
    "\n",
    "def get_masks_only(boxes, image_source, image_rgb):\n",
    "    box = boxes[0].cpu().numpy()\n",
    "    box_pixel = box_to_pixel(box, image_source.shape)\n",
    "\n",
    "    sam_predictor.set_image(image_rgb)\n",
    "    masks, scores, _ = sam_predictor.predict(\n",
    "        box=box_pixel,\n",
    "        multimask_output=True\n",
    "    )\n",
    "\n",
    "    best_mask = masks[np.argmax(scores)]\n",
    "\n",
    "    return (best_mask.astype(np.uint8)) * 255\n",
    "\n",
    "def auto_mask(image_source, base_name):\n",
    "    mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "    masks = mask_generator.generate(image_source)\n",
    "\n",
    "    # Pick the largest mask (or you could use CLIP scoring here)\n",
    "    best_mask = sorted(masks, key=lambda x: x['area'], reverse=True)[0]['segmentation']\n",
    "    auto_mask_render = extract_segmented_object(image_source, best_mask)\n",
    "\n",
    "    cv2.imwrite(os.path.join(OUTPUT_DIR, f\"{base_name}_fallback_overlay.png\"), auto_mask_render)\n",
    "    return auto_mask_render\n",
    "\n",
    "def segment_and_save_views():\n",
    "    \"\"\"Segment all views and save results as images.\"\"\"\n",
    "    view_files = sorted([f for f in os.listdir(VIEWS_DIR) if f.endswith(('.png', '.jpg'))])\n",
    "    \n",
    "    if not view_files:\n",
    "        print(f\"No images found in {VIEWS_DIR}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(view_files)} views to process\")\n",
    "    \n",
    "    for view_file in view_files:\n",
    "        print(f\"Processing {view_file}...\")\n",
    "        view_path = os.path.join(VIEWS_DIR, view_file)\n",
    "        \n",
    "        try:\n",
    "            # Load and prepare image\n",
    "            image_source, image = load_image(view_path)\n",
    "            image_rgb = cv2.cvtColor(image_source, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Get boxes from GroundingDINO\n",
    "            boxes, logits, _ = predict(\n",
    "                model=groundingdino_model,\n",
    "                image=image,\n",
    "                caption=TEXT_PROMPT,\n",
    "                box_threshold=BOX_THRESHOLD,\n",
    "                text_threshold=TEXT_THRESHOLD,\n",
    "                device=DEVICE\n",
    "            )\n",
    "            boxes = rerank_boxes_with_clip(image_source, boxes, TEXT_PROMPT, top_k=1)\n",
    "\n",
    "            if len(boxes) == 0:\n",
    "                print(f\"No objects detected in {view_file}\")\n",
    "                continue\n",
    "            \n",
    "\n",
    "            # Save results\n",
    "            base_name = os.path.splitext(view_file)[0]\n",
    "\n",
    "            # Save annotation with boxes\n",
    "            annotated = annotate(\n",
    "                image_source=image_source,\n",
    "                boxes=boxes,\n",
    "                logits=logits,\n",
    "                phrases=[TEXT_PROMPT]*len(boxes)\n",
    "            )\n",
    "            annotated = annotated[...,::-1]\n",
    "\n",
    "            segmented_frame_masks = segment(image_source, sam_predictor, boxes=boxes)\n",
    "            fused_mask = segmented_frame_masks[0].float().mean(dim=0).cpu().numpy()\n",
    "            binary_mask = (fused_mask > 0.5).astype(np.uint8) * 255\n",
    "\n",
    "            annotated_frame_with_mask = draw_mask(segmented_frame_masks[0][0], annotated)\n",
    "            masked = get_masks_only(boxes, image_source, image_rgb)\n",
    "\n",
    "            cv2.imwrite(os.path.join(OUTPUT_DIR, f\"{base_name}_annotated.png\"), annotated)\n",
    "            cv2.imwrite(os.path.join(OUTPUT_DIR, f\"{base_name}_highlighted.png\"), annotated_frame_with_mask)\n",
    "            highlighted_on_original = extract_segmented_object(image_source, binary_mask)#masked\n",
    "            cv2.imwrite(os.path.join(OUTPUT_DIR, f\"{base_name}_masked_overlay.png\"), highlighted_on_original)\n",
    "\n",
    "            auto_mask(image_source, base_name)\n",
    "\n",
    "            print(f\"Saved results for {view_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {view_file}: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    segment_and_save_views()\n",
    "\n",
    "    print(\"Segmentation complete! Check the output directory for results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3872d9b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
