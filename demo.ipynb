{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "722b997b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "UserWarning: Failed to load custom C++ ops. Running on CPU mode Only!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import open3d as o3d\n",
    "import torch.nn.functional as F\n",
    "import supervision as sv\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from Grounded_Segment_Anything.GroundingDINO.groundingdino.util import box_ops\n",
    "from Grounded_Segment_Anything.GroundingDINO.groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "from Grounded_Segment_Anything.segment_anything.segment_anything import sam_model_registry, SamPredictor, SamAutomaticMaskGenerator\n",
    "\n",
    "\n",
    "CONFIG_PATH = \"./Grounded_Segment_Anything/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
    "CHECKPOINT_PATH = \"./models/groundingdino_swint_ogc.pth\"\n",
    "SAM_CHECKPOINT = \"./models/sam_vit_h_4b8939.pth\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "TEXT_PROMPT = \"ears\"\n",
    "BOX_THRESHOLD = 0.3\n",
    "TEXT_THRESHOLD = 0.25\n",
    "VIEWS_DIR = \"./data\"\n",
    "OUTPUT_DIR = \"./evaluation/real\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab3ed09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3638.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n",
      "Found 7 views to process\n",
      "Processing cat.png...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "SupervisionWarnings: annotate is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n",
      "DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results for cat.png\n",
      "Processing greyscale_cat.png...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SupervisionWarnings: annotate is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results for greyscale_cat.png\n",
      "Processing greyscale_image.png...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SupervisionWarnings: annotate is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results for greyscale_image.png\n",
      "Processing greyscale_orange.png...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SupervisionWarnings: annotate is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results for greyscale_orange.png\n",
      "Processing image.png...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SupervisionWarnings: annotate is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results for image.png\n",
      "Processing orange.png...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SupervisionWarnings: annotate is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results for orange.png\n",
      "Processing rabbit.jpg...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SupervisionWarnings: annotate is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results for rabbit.jpg\n",
      "Segmentation complete! Check the output directory for results.\n"
     ]
    }
   ],
   "source": [
    "groundingdino_model = load_model(CONFIG_PATH, CHECKPOINT_PATH).to(DEVICE)\n",
    "sam = sam_model_registry[\"vit_h\"](checkpoint=SAM_CHECKPOINT).to(DEVICE)\n",
    "sam_predictor = SamPredictor(sam)\n",
    "\n",
    "def segment(image, sam_model, boxes):\n",
    "  sam_model.set_image(image)\n",
    "  H, W, _ = image.shape\n",
    "  boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H])\n",
    "\n",
    "  transformed_boxes = sam_model.transform.apply_boxes_torch(boxes_xyxy.to(DEVICE), image.shape[:2])\n",
    "  masks, _, _ = sam_model.predict_torch(\n",
    "      point_coords = None,\n",
    "      point_labels = None,\n",
    "      boxes = transformed_boxes,\n",
    "      multimask_output = False,\n",
    "      )\n",
    "  return masks.cpu()\n",
    "  \n",
    "\n",
    "def draw_mask(mask, image, random_color=True):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.8])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    \n",
    "    annotated_frame_pil = Image.fromarray(image).convert(\"RGBA\")\n",
    "    mask_image_pil = Image.fromarray((mask_image.cpu().numpy() * 255).astype(np.uint8)).convert(\"RGBA\")\n",
    "\n",
    "    return np.array(Image.alpha_composite(annotated_frame_pil, mask_image_pil))\n",
    "\n",
    "def extract_segmented_object(image, mask):\n",
    "    \"\"\"Extracts only the masked object from the image (black background).\"\"\"\n",
    "    binary_mask = (mask > 0).astype(np.uint8)\n",
    "\n",
    "    # Apply the mask to each channel\n",
    "    segmented = cv2.bitwise_and(image, image, mask=binary_mask)\n",
    "\n",
    "    return segmented\n",
    "\n",
    "def box_to_pixel(box, image_shape):\n",
    "    h, w = image_shape[:2]\n",
    "    cx, cy, bw, bh = box\n",
    "    x1 = int((cx - bw / 2) * w)\n",
    "    y1 = int((cy - bh / 2) * h)\n",
    "    x2 = int((cx + bw / 2) * w)\n",
    "    y2 = int((cy + bh / 2) * h)\n",
    "    return np.array([x1, y1, x2, y2])\n",
    "\n",
    "def get_masks_only(boxes, image_source, image_rgb):\n",
    "    if isinstance(image_rgb, torch.Tensor):\n",
    "        image_rgb = image_rgb.permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    image_rgb = cv2.resize(image_rgb, (image_source.shape[1], image_source.shape[0]))\n",
    "\n",
    "    sam_predictor.set_image(image_rgb)\n",
    "\n",
    "    all_masks = np.zeros(image_source.shape[:2], dtype=np.uint8)\n",
    "\n",
    "    for box_tensor in boxes:\n",
    "        box = box_tensor.cpu().numpy()\n",
    "        box_pixel = box_to_pixel(box, image_source.shape)\n",
    "\n",
    "        masks, scores, _ = sam_predictor.predict(\n",
    "            box=box_pixel,\n",
    "            multimask_output=True\n",
    "        )\n",
    "\n",
    "        best_mask = masks[np.argmax(scores)]\n",
    "        all_masks = np.maximum(all_masks, (best_mask.astype(np.uint8)) * 255)\n",
    "\n",
    "    return all_masks\n",
    "\n",
    "def auto_mask(image_source, base_name):\n",
    "    mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "    masks = mask_generator.generate(image_source)\n",
    "\n",
    "    best_mask = sorted(masks, key=lambda x: x['area'], reverse=True)[0]['segmentation']\n",
    "    auto_mask_render = extract_segmented_object(image_source, best_mask)\n",
    "\n",
    "    cv2.imwrite(os.path.join(OUTPUT_DIR, f\"{base_name}_fallback_overlay.png\"), auto_mask_render)\n",
    "    return auto_mask_render\n",
    "\n",
    "def segment_and_save_views():\n",
    "    \"\"\"Segment all views and save results as images.\"\"\"\n",
    "    view_files = sorted([f for f in os.listdir(VIEWS_DIR) if f.endswith(('.png', '.jpg'))])\n",
    "    \n",
    "    if not view_files:\n",
    "        print(f\"No images found in {VIEWS_DIR}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(view_files)} views to process\")\n",
    "    \n",
    "    for view_file in view_files:\n",
    "        print(f\"Processing {view_file}...\")\n",
    "        view_path = os.path.join(VIEWS_DIR, view_file)\n",
    "        \n",
    "        try:\n",
    "            # Load and prepare image\n",
    "            image_source, image = load_image(view_path)\n",
    "\n",
    "            # Get boxes from GroundingDINO\n",
    "            boxes, logits, phrases = predict(\n",
    "                model=groundingdino_model,\n",
    "                image=image,\n",
    "                caption=TEXT_PROMPT,\n",
    "                box_threshold=BOX_THRESHOLD,\n",
    "                text_threshold=TEXT_THRESHOLD,\n",
    "                device=DEVICE\n",
    "            )\n",
    "\n",
    "            if len(boxes) == 0:\n",
    "                print(f\"No object parts detected in {view_file}\")\n",
    "                continue\n",
    "\n",
    "            # Save results\n",
    "            base_name = os.path.splitext(view_file)[0]\n",
    "\n",
    "            # Save annotation with boxes\n",
    "            annotated = annotate(\n",
    "                image_source=image_source,\n",
    "                boxes=boxes,\n",
    "                logits=logits,\n",
    "                phrases=phrases\n",
    "            )\n",
    "            annotated = annotated[...,::-1]\n",
    "\n",
    "            segmented_frame_masks = segment(image_source, sam_predictor, boxes=boxes)\n",
    "            mask = segmented_frame_masks[0][0].cpu().numpy()\n",
    "            annotated_frame_with_mask = draw_mask(segmented_frame_masks[0][0], annotated)\n",
    "            binary_mask = (mask > 0).astype(np.uint8) * 255\n",
    "\n",
    "            cv2.imwrite(os.path.join(OUTPUT_DIR, f\"{base_name}_annotated.png\"), annotated)\n",
    "            cv2.imwrite(os.path.join(OUTPUT_DIR, f\"{base_name}_highlighted.png\"), annotated_frame_with_mask)\n",
    "            highlighted_on_original = extract_segmented_object(image_source, binary_mask)\n",
    "            cv2.imwrite(os.path.join(OUTPUT_DIR, f\"{base_name}_masked_overlay.png\"), highlighted_on_original)\n",
    "\n",
    "            #auto_mask(image_source, base_name)\n",
    "\n",
    "            print(f\"Saved results for {view_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {view_file}: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    segment_and_save_views()\n",
    "\n",
    "    print(\"Segmentation complete! Check the output directory for results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baac9d9e",
   "metadata": {},
   "source": [
    "# Greyscale Image Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ea2970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n",
      "Found 6 views to process\n",
      "Processing cat.png...\n",
      "No objects detected in cat.png\n",
      "Processing greyscale_cat.png...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SupervisionWarnings: annotate is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n",
      "DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results for greyscale_cat.png\n",
      "Processing greyscale_image.png...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SupervisionWarnings: annotate is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results for greyscale_image.png\n",
      "Processing greyscale_orange.png...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SupervisionWarnings: annotate is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results for greyscale_orange.png\n",
      "Processing image.png...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SupervisionWarnings: annotate is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results for image.png\n",
      "Processing orange.png...\n",
      "No objects detected in orange.png\n",
      "Segmentation complete! Check the output directory for results.\n"
     ]
    }
   ],
   "source": [
    "CONFIG_PATH = \"./Grounded_Segment_Anything/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
    "CHECKPOINT_PATH = \"./models/groundingdino_swint_ogc.pth\"\n",
    "SAM_CHECKPOINT = \"./models/sam_vit_h_4b8939.pth\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "TEXT_PROMPT = \"ear\"\n",
    "BOX_THRESHOLD = 0.3\n",
    "TEXT_THRESHOLD = 0.25\n",
    "VIEWS_DIR = \"./data\"\n",
    "OUTPUT_DIR = \"./evaluation/cat_images/\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "groundingdino_model = load_model(CONFIG_PATH, CHECKPOINT_PATH).to(DEVICE)\n",
    "sam = sam_model_registry[\"vit_h\"](checkpoint=SAM_CHECKPOINT).to(DEVICE)\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(DEVICE)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "sam_predictor = SamPredictor(sam)\n",
    "\n",
    "def rerank_boxes_with_clip(image_source, boxes, text_prompt, top_k=1):\n",
    "    \"\"\"\n",
    "    Refine box selection using CLIP image-text similarity.\n",
    "    \"\"\"\n",
    "    H, W, _ = image_source.shape\n",
    "    boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.tensor([W, H, W, H], device=boxes.device)\n",
    "    boxes_xyxy = boxes_xyxy.int()\n",
    "\n",
    "    cropped_images = []\n",
    "    valid_indices = []\n",
    "    for i, box in enumerate(boxes_xyxy.view(-1, 4)):\n",
    "        x1, y1, x2, y2 = box.tolist()\n",
    "        crop = image_source[y1:y2, x1:x2]\n",
    "        if crop.size == 0:\n",
    "            continue\n",
    "        crop_pil = Image.fromarray(crop).convert(\"RGB\")\n",
    "        cropped_images.append(crop_pil)\n",
    "        valid_indices.append(i)\n",
    "\n",
    "    if not cropped_images:\n",
    "        return torch.empty((0, 4), dtype=boxes.dtype).to(boxes.device)\n",
    "\n",
    "    inputs = clip_processor(\n",
    "        text=[text_prompt] * len(cropped_images),\n",
    "        images=cropped_images,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = clip_model(**inputs)\n",
    "        logits_per_image = outputs.logits_per_image  # shape [N, 1]\n",
    "        probs = F.softmax(logits_per_image.squeeze(), dim=0)\n",
    "\n",
    "    top_indices = probs.topk(top_k).indices\n",
    "\n",
    "    selected_indices = [valid_indices[i] for i in top_indices.cpu()]\n",
    "    filtered_boxes = boxes[selected_indices]\n",
    "\n",
    "    filtered_boxes = filtered_boxes.view(-1, 4)\n",
    "\n",
    "    return filtered_boxes\n",
    "\n",
    "def segment(image, sam_model, boxes):\n",
    "  sam_model.set_image(image)\n",
    "  H, W, _ = image.shape\n",
    "  boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H])\n",
    "\n",
    "  transformed_boxes = sam_model.transform.apply_boxes_torch(boxes_xyxy.to(DEVICE), image.shape[:2])\n",
    "  masks, _, _ = sam_model.predict_torch(\n",
    "      point_coords = None,\n",
    "      point_labels = None,\n",
    "      boxes = transformed_boxes,\n",
    "      multimask_output = False,\n",
    "      )\n",
    "  return masks.cpu()\n",
    "  \n",
    "\n",
    "def draw_mask(mask, image, random_color=True):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.8])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    \n",
    "    annotated_frame_pil = Image.fromarray(image).convert(\"RGBA\")\n",
    "    mask_image_pil = Image.fromarray((mask_image.cpu().numpy() * 255).astype(np.uint8)).convert(\"RGBA\")\n",
    "\n",
    "    return np.array(Image.alpha_composite(annotated_frame_pil, mask_image_pil))\n",
    "\n",
    "def extract_segmented_object(image, mask):\n",
    "    \"\"\"Extracts only the masked object from the image (black background).\"\"\"\n",
    "    binary_mask = (mask > 0).astype(np.uint8)\n",
    "\n",
    "    # Apply the mask to each channel\n",
    "    segmented = cv2.bitwise_and(image, image, mask=binary_mask)\n",
    "\n",
    "    return segmented\n",
    "\n",
    "def box_to_pixel(box, image_shape):\n",
    "    h, w = image_shape[:2]\n",
    "    cx, cy, bw, bh = box\n",
    "    x1 = int((cx - bw / 2) * w)\n",
    "    y1 = int((cy - bh / 2) * h)\n",
    "    x2 = int((cx + bw / 2) * w)\n",
    "    y2 = int((cy + bh / 2) * h)\n",
    "    return np.array([x1, y1, x2, y2])\n",
    "\n",
    "def get_masks_only(boxes, image_source, image_rgb):\n",
    "    if isinstance(image_rgb, torch.Tensor):\n",
    "        image_rgb = image_rgb.permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    image_rgb = cv2.resize(image_rgb, (image_source.shape[1], image_source.shape[0]))\n",
    "\n",
    "    sam_predictor.set_image(image_rgb)\n",
    "\n",
    "    all_masks = np.zeros(image_source.shape[:2], dtype=np.uint8)\n",
    "\n",
    "    for box_tensor in boxes:\n",
    "        box = box_tensor.cpu().numpy()\n",
    "        box_pixel = box_to_pixel(box, image_source.shape)\n",
    "\n",
    "        masks, scores, _ = sam_predictor.predict(\n",
    "            box=box_pixel,\n",
    "            multimask_output=True\n",
    "        )\n",
    "\n",
    "        best_mask = masks[np.argmax(scores)]\n",
    "        all_masks = np.maximum(all_masks, (best_mask.astype(np.uint8)) * 255)\n",
    "\n",
    "    return all_masks\n",
    "\n",
    "def auto_mask(image_source, base_name):\n",
    "    mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "    masks = mask_generator.generate(image_source)\n",
    "\n",
    "    # Pick the largest mask (or you could use CLIP scoring here)\n",
    "    best_mask = sorted(masks, key=lambda x: x['area'], reverse=True)[0]['segmentation']\n",
    "    auto_mask_render = extract_segmented_object(image_source, best_mask)\n",
    "\n",
    "    cv2.imwrite(os.path.join(OUTPUT_DIR, f\"{base_name}_fallback_overlay.png\"), auto_mask_render)\n",
    "    return auto_mask_render\n",
    "\n",
    "def segment_and_save_views():\n",
    "    \"\"\"Segment all views and save results as images.\"\"\"\n",
    "    view_files = sorted([f for f in os.listdir(VIEWS_DIR) if f.endswith(('.png', '.jpg'))])\n",
    "    \n",
    "    if not view_files:\n",
    "        print(f\"No images found in {VIEWS_DIR}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(view_files)} views to process\")\n",
    "    \n",
    "    for view_file in view_files:\n",
    "        print(f\"Processing {view_file}...\")\n",
    "        view_path = os.path.join(VIEWS_DIR, view_file)\n",
    "        \n",
    "        try:\n",
    "            # Load and prepare image\n",
    "            image_source, image = load_image(view_path)\n",
    "\n",
    "            # Get boxes from GroundingDINO\n",
    "            boxes, logits, phrases = predict(\n",
    "                model=groundingdino_model,\n",
    "                image=image,\n",
    "                caption=TEXT_PROMPT,\n",
    "                box_threshold=BOX_THRESHOLD,\n",
    "                text_threshold=TEXT_THRESHOLD,\n",
    "                device=DEVICE\n",
    "            )\n",
    "            #boxes = rerank_boxes_with_clip(image_source, boxes, TEXT_PROMPT, top_k=1)\n",
    "\n",
    "            if len(boxes) == 0:\n",
    "                print(f\"No objects detected in {view_file}\")\n",
    "                continue\n",
    "\n",
    "            # Save results\n",
    "            base_name = os.path.splitext(view_file)[0]\n",
    "\n",
    "            # Save annotation with boxes\n",
    "            annotated = annotate(\n",
    "                image_source=image_source,\n",
    "                boxes=boxes,\n",
    "                logits=logits,\n",
    "                phrases=phrases\n",
    "            )\n",
    "            annotated = annotated[...,::-1]\n",
    "\n",
    "            segmented_frame_masks = segment(image_source, sam_predictor, boxes=boxes)\n",
    "            \n",
    "            annotated_frame_with_mask = draw_mask(segmented_frame_masks[0][0], annotated)\n",
    "            masked = get_masks_only(boxes, image_source, image)\n",
    "\n",
    "            cv2.imwrite(os.path.join(OUTPUT_DIR, f\"{base_name}_annotated.png\"), annotated)\n",
    "            cv2.imwrite(os.path.join(OUTPUT_DIR, f\"{base_name}_highlighted.png\"), annotated_frame_with_mask)\n",
    "            highlighted_on_original = extract_segmented_object(image_source, masked)#masked\n",
    "            cv2.imwrite(os.path.join(OUTPUT_DIR, f\"{base_name}_masked_overlay.png\"), highlighted_on_original)\n",
    "\n",
    "            #auto_mask(image_source, base_name)\n",
    "\n",
    "            print(f\"Saved results for {view_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {view_file}: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    segment_and_save_views()\n",
    "\n",
    "    print(\"Segmentation complete! Check the output directory for results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c49626",
   "metadata": {},
   "source": [
    "# LLM Input Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "699f34f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.97s/it]\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "def refine_input(user_input, object_caption, llm):\n",
    "    # System prompt for the refinement task\n",
    "    refinement_prompt = f\"\"\"\n",
    "    You are a 3D object segmentation assistant. Your task is to:\n",
    "    1. Extract the core component/part name (e.g., \"ear\" from \"I want to segment bunny's ear\")\n",
    "    2. Analyze the user's requested object part against the actual object description and validate if the part makes sense for this object and if not output \"None\" (e.g., \"ear\" from \"I want to segment chairs ear\").\n",
    "    3. If there are mutliple parts, return the parts with a dot following one other (e.g., \"ear. leg.\" from \"I want to segment bunny's ear and leg\").\n",
    "    \n",
    "    Object Description: {object_caption}\n",
    "    User Request: {user_input}\n",
    "    \n",
    "    Examples:\n",
    "    1. If object is \"a chair\" and request is \"the back support\":\n",
    "    <Part>back support</Part>\n",
    "    <Validation>Segmenting the back support from the chair.</Validation>\n",
    "    \n",
    "    2. If object is \"a car\" and request is \"the ears\":\n",
    "    <Part>None</Part>\n",
    "    <Validation>Cars don't have ears. Did you mean mirrors, antennas, or another part?</Validation>\n",
    "\n",
    "    3. If object is \"a bunny\" and request is \"the ear and the leg\":\n",
    "    <Part>ear. leg.</Part>\n",
    "    <Validation>Segmenting the ear and leg from the bunny.</Validation>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get LLM response\n",
    "    response = llm(refinement_prompt, max_new_tokens=256, do_sample=False)[0][\"generated_text\"]\n",
    "\n",
    "    # Parse the response\n",
    "    try:\n",
    "        part = response.split(\"<Part>\")[1].split(\"</Part>\")[0].strip().lower()\n",
    "        print('PART', part)\n",
    "        validation = response.split(\"<Validation>\")[1].split(\"</Validation>\")[0].strip()\n",
    "        print('VALIDATION', validation)\n",
    "        if validation.lower() == \"none\":\n",
    "            validation = None\n",
    "    except:\n",
    "        part = user_input\n",
    "        validation = \"Could not parse response - using original input\"\n",
    "    \n",
    "    # Handle cases where no valid part was extracted\n",
    "    if part.lower() == \"none\":\n",
    "        return None, validation\n",
    "    \n",
    "    return part, validation\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"meta-llama/Llama-3.2-3B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fdbbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PART back support\n",
      "VALIDATION Segmenting the back support from the chair.\n",
      "Extracted Part: back support\n",
      "Validation: Segmenting the back support from the chair.\n"
     ]
    }
   ],
   "source": [
    "user_input = \"I want to segment the chair leg\"\n",
    "object_caption = \"a chair with a seat and back\"\n",
    "part, validation = refine_input(user_input, object_caption, pipe)\n",
    "print(\"Extracted Part:\", part)\n",
    "print(\"Validation:\", validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3872d9b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread('./data/image.png')\n",
    "\n",
    "# Convert to greyscale\n",
    "grey_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Save the greyscale image\n",
    "cv2.imwrite('./data/greyscale_image.png', grey_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9388ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
