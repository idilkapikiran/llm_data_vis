{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af49dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import open3d as o3d\n",
    "import supervision as sv\n",
    "from Grounded_Segment_Anything.GroundingDINO.groundingdino.util import box_ops\n",
    "from Grounded_Segment_Anything.GroundingDINO.groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "from Grounded_Segment_Anything.segment_anything.segment_anything import sam_model_registry, SamPredictor\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "CONFIG_PATH = \"./Grounded_Segment_Anything/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
    "CHECKPOINT_PATH = \"./models/groundingdino_swint_ogc.pth\"\n",
    "SAM_CHECKPOINT = \"./models/sam_vit_h_4b8939.pth\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "TEXT_PROMPT = \"bunny ear\"\n",
    "BOX_THRESHOLD = 0.3\n",
    "TEXT_THRESHOLD = 0.25\n",
    "VIEWS_DIR = \"./render_views/bunny\"\n",
    "OUTPUT_DIR = \"./new/new_bunny\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd5690c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n",
      "Found 5 views to process\n",
      "Processing view_0.png...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "SupervisionWarnings: annotate is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n",
      "DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results for view_0.png\n",
      "Processing view_1.png...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SupervisionWarnings: annotate is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results for view_1.png\n",
      "Processing view_2.png...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SupervisionWarnings: annotate is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results for view_2.png\n",
      "Processing view_3.png...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SupervisionWarnings: annotate is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results for view_3.png\n",
      "Processing view_4.png...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SupervisionWarnings: annotate is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results for view_4.png\n",
      "Segmentation complete! Check the output directory for results.\n"
     ]
    }
   ],
   "source": [
    "groundingdino_model = load_model(CONFIG_PATH, CHECKPOINT_PATH).to(DEVICE)\n",
    "sam = sam_model_registry[\"vit_h\"](checkpoint=SAM_CHECKPOINT).to(DEVICE)\n",
    "sam_predictor = SamPredictor(sam)\n",
    "view_list = []\n",
    "\n",
    "def segment(image, sam_model, boxes):\n",
    "  sam_model.set_image(image)\n",
    "  H, W, _ = image.shape\n",
    "  boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H])\n",
    "\n",
    "  transformed_boxes = sam_model.transform.apply_boxes_torch(boxes_xyxy.to(DEVICE), image.shape[:2])\n",
    "  masks, _, _ = sam_model.predict_torch(\n",
    "      point_coords = None,\n",
    "      point_labels = None,\n",
    "      boxes = transformed_boxes,\n",
    "      multimask_output = False,\n",
    "      )\n",
    "  return masks.cpu()\n",
    "  \n",
    "\n",
    "def draw_mask(mask, image, random_color=True):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.8])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    \n",
    "    annotated_frame_pil = Image.fromarray(image).convert(\"RGBA\")\n",
    "    mask_image_pil = Image.fromarray((mask_image.cpu().numpy() * 255).astype(np.uint8)).convert(\"RGBA\")\n",
    "\n",
    "    return np.array(Image.alpha_composite(annotated_frame_pil, mask_image_pil))\n",
    "\n",
    "def extract_segmented_object(image, mask):\n",
    "    \"\"\"Extracts only the masked object from the image (black background).\"\"\"\n",
    "    # Ensure mask is binary (0 or 1)\n",
    "    binary_mask = (mask > 0).astype(np.uint8)\n",
    "\n",
    "    # Apply the mask to each channel\n",
    "    segmented = cv2.bitwise_and(image, image, mask=binary_mask)\n",
    "\n",
    "    return segmented\n",
    "\n",
    "def box_to_pixel(box, image_shape):\n",
    "    h, w = image_shape[:2]\n",
    "    cx, cy, bw, bh = box\n",
    "    x1 = int((cx - bw / 2) * w)\n",
    "    y1 = int((cy - bh / 2) * h)\n",
    "    x2 = int((cx + bw / 2) * w)\n",
    "    y2 = int((cy + bh / 2) * h)\n",
    "    return np.array([x1, y1, x2, y2])\n",
    "\n",
    "def get_masks_only(boxes, image_source, image_rgb):\n",
    "    box = boxes[0].cpu().numpy()\n",
    "    box_pixel = box_to_pixel(box, image_source.shape)\n",
    "\n",
    "    sam_predictor.set_image(image_rgb)\n",
    "    masks, scores, _ = sam_predictor.predict(\n",
    "        box=box_pixel,\n",
    "        multimask_output=True\n",
    "    )\n",
    "\n",
    "    best_mask = masks[np.argmax(scores)]\n",
    "\n",
    "    return (best_mask.astype(np.uint8)) * 255\n",
    "\n",
    "\n",
    "def segment_and_save_views():\n",
    "    \"\"\"Segment all views and save results as images.\"\"\"\n",
    "    view_files = sorted([f for f in os.listdir(VIEWS_DIR) if f.endswith(('.png', '.jpg'))])\n",
    "    \n",
    "    if not view_files:\n",
    "        print(f\"No images found in {VIEWS_DIR}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(view_files)} views to process\")\n",
    "    \n",
    "    for view_file in view_files:\n",
    "        print(f\"Processing {view_file}...\")\n",
    "        view_path = os.path.join(VIEWS_DIR, view_file)\n",
    "        \n",
    "        try:\n",
    "            # Load and prepare image\n",
    "            image_source, image = load_image(view_path)\n",
    "            image_rgb = cv2.cvtColor(image_source, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Get boxes from GroundingDINO\n",
    "            boxes, logits, _ = predict(\n",
    "                model=groundingdino_model,\n",
    "                image=image,\n",
    "                caption=TEXT_PROMPT,\n",
    "                box_threshold=BOX_THRESHOLD,\n",
    "                text_threshold=TEXT_THRESHOLD,\n",
    "                device=DEVICE\n",
    "            )\n",
    "            \n",
    "            if len(boxes) == 0:\n",
    "                print(f\"No objects detected in {view_file}\")\n",
    "                continue\n",
    "            \n",
    "\n",
    "            # Save results\n",
    "            base_name = os.path.splitext(view_file)[0]\n",
    "\n",
    "            # Save annotation with boxes\n",
    "            annotated = annotate(\n",
    "                image_source=image_source,\n",
    "                boxes=boxes,\n",
    "                logits=logits,\n",
    "                phrases=[TEXT_PROMPT]*len(boxes)\n",
    "            )\n",
    "\n",
    "            segmented_frame_masks = segment(image_source, sam_predictor, boxes=boxes)\n",
    "            annotated_frame_with_mask = draw_mask(segmented_frame_masks[0][0], annotated)\n",
    "            masked = get_masks_only(boxes, image_source, image_rgb)\n",
    "\n",
    "            cv2.imwrite(os.path.join(OUTPUT_DIR, f\"{base_name}_annotated.png\"), annotated)\n",
    "            cv2.imwrite(os.path.join(OUTPUT_DIR, f\"{base_name}_highlighted.png\"), annotated_frame_with_mask)\n",
    "            highlighted_on_original = extract_segmented_object(image_source, masked)\n",
    "            cv2.imwrite(os.path.join(OUTPUT_DIR, f\"{base_name}_masked_overlay.png\"), highlighted_on_original)\n",
    "\n",
    "            \n",
    "            view_list.append(highlighted_on_original)\n",
    "\n",
    "            print(f\"Saved results for {view_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {view_file}: {str(e)}\")\n",
    "\n",
    "    return view_list\n",
    "if __name__ == \"__main__\":\n",
    "    segment_and_save_views()\n",
    "\n",
    "    print(\"Segmentation complete! Check the output directory for results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1ba042",
   "metadata": {},
   "source": [
    "## 3d fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7295c9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "groundingdino_model = load_model(CONFIG_PATH, CHECKPOINT_PATH).to(DEVICE)\n",
    "sam = sam_model_registry[\"vit_h\"](checkpoint=SAM_CHECKPOINT).to(DEVICE)\n",
    "sam_predictor = SamPredictor(sam)\n",
    "\n",
    "def segment_views():\n",
    "    \"\"\"Segment all views using GroundingDINO + SAM\"\"\"\n",
    "    masks = []\n",
    "    view_files = sorted([f for f in os.listdir(VIEWS_DIR) if f.endswith('.png')])\n",
    "    \n",
    "    for view_file in view_files:\n",
    "        # Load image\n",
    "        view_path = os.path.join(VIEWS_DIR, view_file)\n",
    "        image_source, image = load_image(view_path)\n",
    "        \n",
    "        # Get boxes from GroundingDINO\n",
    "        boxes, logits, _ = predict(\n",
    "            model=groundingdino_model,\n",
    "            image=image,\n",
    "            caption=TEXT_PROMPT,\n",
    "            box_threshold=BOX_THRESHOLD,\n",
    "            text_threshold=TEXT_THRESHOLD\n",
    "        )\n",
    "        \n",
    "        if len(boxes) == 0:\n",
    "            print(f\"No objects detected in {view_file}\")\n",
    "            masks.append(np.zeros(image_source.shape[:2], dtype=np.uint8))\n",
    "            continue\n",
    "        \n",
    "        # Get masks from SAM\n",
    "        sam_predictor.set_image(image_source)\n",
    "        box = boxes[0].cpu().numpy()  # Use the highest-scoring box\n",
    "        mask, _, _ = sam_predictor.predict(box=box)\n",
    "        mask = mask[0].astype(np.uint8) * 255\n",
    "        \n",
    "        masks.append(mask)\n",
    "        cv2.imwrite(os.path.join(OUTPUT_DIR, f\"mask_{view_file}\"), mask)\n",
    "        \n",
    "        # Save visualization (optional)\n",
    "        annotated = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=[TEXT_PROMPT]*len(boxes))\n",
    "        cv2.imwrite(os.path.join(OUTPUT_DIR, f\"annotated_{view_file}\"), annotated)\n",
    "    \n",
    "    return masks\n",
    "\n",
    "def reconstruct_3d(masks, camera_poses, K, voxel_size=0.01):\n",
    "    \"\"\"Reconstruct 3D from masks using TSDF fusion\"\"\"\n",
    "    volume = o3d.pipelines.integration.ScalableTSDFVolume(\n",
    "        voxel_length=voxel_size,\n",
    "        sdf_trunc=3 * voxel_size,\n",
    "        color_type=o3d.pipelines.integration.TSDFVolumeColorType.RGB8,\n",
    "    )\n",
    "    \n",
    "    for i, (mask, pose) in enumerate(zip(masks, camera_poses)):\n",
    "        # Create depth map (assuming mask bounds the object)\n",
    "        depth = np.where(mask > 0, 0.5, 0)  # Replace with actual depth if available\n",
    "        \n",
    "        # Convert to Open3D types\n",
    "        color = o3d.io.read_image(os.path.join(VIEWS_DIR, f\"view_{i}.png\"))\n",
    "        depth = o3d.geometry.Image(depth.astype(np.float32))\n",
    "        \n",
    "        # Integrate into TSDF volume\n",
    "        intrinsic = o3d.camera.PinholeCameraIntrinsic(\n",
    "            width=masks[0].shape[1],\n",
    "            height=masks[0].shape[0],\n",
    "            fx=K[0,0], fy=K[1,1],\n",
    "            cx=K[0,2], cy=K[1,2],\n",
    "        )\n",
    "        extrinsic = np.linalg.inv(pose)  # Camera-to-world transform\n",
    "        volume.integrate(\n",
    "            o3d.geometry.RGBDImage.create_from_color_and_depth(\n",
    "                color, depth, depth_scale=1.0, depth_trunc=1.0, convert_rgb_to_intensity=False\n",
    "            ),\n",
    "            intrinsic,\n",
    "            extrinsic,\n",
    "        )\n",
    "    \n",
    "    return volume.extract_triangle_mesh()\n",
    "\n",
    "# Main pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Segment all views\n",
    "    masks = segment_views()\n",
    "    \n",
    "    # 2. Load/generate camera poses (replace with your actual poses)\n",
    "    num_views = len(masks)\n",
    "    camera_poses = []\n",
    "    for i in range(num_views):\n",
    "        angle = 2 * np.pi * i / num_views\n",
    "        pose = np.eye(4)\n",
    "        pose[:3, 3] = [np.cos(angle), np.sin(angle), 0.5]  # Circular path\n",
    "        camera_poses.append(pose)\n",
    "    \n",
    "    # 3. Create intrinsic matrix (replace with your camera parameters)\n",
    "    K = np.array([\n",
    "        [500, 0, masks[0].shape[1]/2],\n",
    "        [0, 500, masks[0].shape[0]/2],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "    \n",
    "    # 4. Reconstruct 3D\n",
    "    mesh = reconstruct_3d(masks, camera_poses, K)\n",
    "    \n",
    "    # 5. Save and visualize\n",
    "    o3d.io.write_triangle_mesh(\"reconstructed.ply\", mesh)\n",
    "    o3d.visualization.draw_geometries([mesh], mesh_show_wireframe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60c6acee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Configuration\n",
    "IMAGE_WIDTH = 800  # Adjust to your image dimensions\n",
    "IMAGE_HEIGHT = 600\n",
    "FOCAL_LENGTH = 1000  # In pixels\n",
    "DISTANCE_FROM_ORIGIN = 2.0  # Distance from object center\n",
    "ANGLES = [0, 45, 90, 135, 180]  # Your specified angles\n",
    "\n",
    "def generate_camera_parameters():\n",
    "    \"\"\"Generate camera intrinsics and extrinsics for each view\"\"\"\n",
    "    cameras = {}\n",
    "    \n",
    "    # Intrinsic matrix (same for all cameras in this setup)\n",
    "    K = np.array([\n",
    "        [FOCAL_LENGTH, 0, IMAGE_WIDTH/2],\n",
    "        [0, FOCAL_LENGTH, IMAGE_HEIGHT/2],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "    \n",
    "    # Generate extrinsic parameters for each angle\n",
    "    for angle in ANGLES:\n",
    "        # Convert angle to radians\n",
    "        theta = np.radians(angle)\n",
    "        \n",
    "        # Camera position (circular path around object)\n",
    "        cam_pos = np.array([\n",
    "            DISTANCE_FROM_ORIGIN * np.sin(theta),\n",
    "            0,\n",
    "            DISTANCE_FROM_ORIGIN * np.cos(theta)\n",
    "        ])\n",
    "        \n",
    "        # Look at origin (object center)\n",
    "        look_at = np.array([0, 0, 0])\n",
    "        \n",
    "        # Up vector (assuming Y is up)\n",
    "        up = np.array([0, 1, 0])\n",
    "        \n",
    "        # Create view matrix (camera extrinsics)\n",
    "        z_axis = (look_at - cam_pos)\n",
    "        z_axis /= np.linalg.norm(z_axis)\n",
    "        x_axis = np.cross(up, z_axis)\n",
    "        x_axis /= np.linalg.norm(x_axis)\n",
    "        y_axis = np.cross(z_axis, x_axis)\n",
    "        \n",
    "        R = np.vstack([x_axis, y_axis, z_axis]).T\n",
    "        t = -R @ cam_pos\n",
    "        \n",
    "        # Create 4x4 transformation matrix\n",
    "        extrinsic = np.eye(4)\n",
    "        extrinsic[:3, :3] = R\n",
    "        extrinsic[:3, 3] = t\n",
    "        \n",
    "        cameras[f\"angle_{angle}\"] = {\n",
    "            \"intrinsic\": K,\n",
    "            \"extrinsic\": extrinsic,\n",
    "            \"position\": cam_pos,\n",
    "            \"rotation\": R\n",
    "        }\n",
    "    \n",
    "    return cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b060e785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_3d_with_known_cameras(views_dir, masks_dir, output_path=\"reconstruction.ply\"):\n",
    "    cameras = generate_camera_parameters()\n",
    "    point_cloud = o3d.geometry.PointCloud()\n",
    "    \n",
    "    mask_files = sorted([f for f in os.listdir(masks_dir) if f.endswith('_mask.png')])\n",
    "    \n",
    "    for mask_file, angle in zip(mask_files, ANGLES):\n",
    "        view_file = mask_file.replace('_mask.png', '.png')\n",
    "        view_path = os.path.join(views_dir, view_file)\n",
    "        mask_path = os.path.join(masks_dir, mask_file)\n",
    "        \n",
    "        # Load images\n",
    "        view = cv2.imread(view_path)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        # Get camera parameters for this angle\n",
    "        cam_data = cameras[f\"angle_{angle}\"]\n",
    "        K = cam_data[\"intrinsic\"]\n",
    "        extrinsic = cam_data[\"extrinsic\"]\n",
    "        \n",
    "        # Create depth map (simplified - assumes flat depth)\n",
    "        # In a real scenario, you'd need proper depth estimation\n",
    "        depth_map = np.where(mask > 128, DISTANCE_FROM_ORIGIN, 0).astype(np.float32)\n",
    "        \n",
    "        # Back-project to 3D using camera parameters\n",
    "        h, w = depth_map.shape\n",
    "        u, v = np.meshgrid(np.arange(w), np.arange(h))\n",
    "        uv_homog = np.stack([u.flatten(), v.flatten(), np.ones_like(u.flatten())], axis=1)\n",
    "        \n",
    "        # Transform to camera coordinates\n",
    "        points_cam = (np.linalg.inv(K) @ uv_homog.T).T * depth_map.flatten()[:, None]\n",
    "        \n",
    "        # Transform to world coordinates\n",
    "        points_world = (extrinsic[:3, :3] @ points_cam.T + extrinsic[:3, [3]]).T\n",
    "        \n",
    "        # Filter valid points\n",
    "        valid_points = points_world[depth_map.flatten() > 0]\n",
    "        \n",
    "        if len(valid_points) > 0:\n",
    "            point_cloud.points.extend(o3d.utility.Vector3dVector(valid_points))\n",
    "    \n",
    "    # Post-processing\n",
    "    point_cloud, _ = point_cloud.remove_statistical_outlier(nb_neighbors=20, std_ratio=2.0)\n",
    "    \n",
    "    # Surface reconstruction\n",
    "    mesh, _ = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(point_cloud)\n",
    "    \n",
    "    # Save result\n",
    "    o3d.io.write_triangle_mesh(output_path, mesh)\n",
    "    print(f\"Saved 3D reconstruction to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb6ef36",
   "metadata": {},
   "source": [
    "## blip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dda03dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n",
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BLIP-2 model...\n",
      "Found 5 views to process\n",
      "Processing view_0.png...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 112\u001b[0m\n\u001b[1;32m    109\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mimwrite(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(OUTPUT_DIR, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_mask.png\u001b[39m\u001b[38;5;124m\"\u001b[39m), annotated_frame_with_mask)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 112\u001b[0m     \u001b[43msegment_and_fuse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSegmentation and volumetric fusion complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 86\u001b[0m, in \u001b[0;36msegment_and_fuse\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m image_source_rgb \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image_source, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m     84\u001b[0m image_pil \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(image_source_rgb)\n\u001b[0;32m---> 86\u001b[0m boxes, logits, _ \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroundingdino_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcaption\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbox_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBOX_THRESHOLD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTEXT_THRESHOLD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(boxes) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects detected in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mview_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/llm_data_vis/Grounded_Segment_Anything/GroundingDINO/groundingdino/util/inference.py:61\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(model, image, caption, box_threshold, text_threshold, device)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m     54\u001b[0m         model,\n\u001b[1;32m     55\u001b[0m         image: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m         device: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     60\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor, List[\u001b[38;5;28mstr\u001b[39m]]:\n\u001b[0;32m---> 61\u001b[0m     caption \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_caption\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcaption\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcaption\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     64\u001b[0m     image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/Documents/GitHub/llm_data_vis/Grounded_Segment_Anything/GroundingDINO/groundingdino/util/inference.py:23\u001b[0m, in \u001b[0;36mpreprocess_caption\u001b[0;34m(caption)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpreprocess_caption\u001b[39m(caption: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m---> 23\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mcaption\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# Initialize models\n",
    "print(\"Loading models...\")\n",
    "groundingdino_model = load_model(CONFIG_PATH, CHECKPOINT_PATH).to(DEVICE)\n",
    "sam = sam_model_registry[\"vit_h\"](checkpoint=SAM_CHECKPOINT).to(DEVICE)\n",
    "sam_predictor = SamPredictor(sam)\n",
    "\n",
    "print(\"Loading BLIP-2 model...\")\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(DEVICE)\n",
    "\n",
    "def generate_prompt_with_blip2(image_pil):\n",
    "    inputs = blip_processor(image_pil, return_tensors=\"pt\").to(DEVICE)\n",
    "    out = blip_model.generate(**inputs, max_new_tokens=20)\n",
    "    caption = blip_processor.decode(out[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "def segment(image, sam_model, boxes):\n",
    "    sam_model.set_image(image)\n",
    "    H, W, _ = image.shape\n",
    "    boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H])\n",
    "    transformed_boxes = sam_model.transform.apply_boxes_torch(boxes_xyxy.to(DEVICE), image.shape[:2])\n",
    "    masks, _, _ = sam_model.predict_torch(\n",
    "        point_coords = None,\n",
    "        point_labels = None,\n",
    "        boxes = transformed_boxes,\n",
    "        multimask_output = False,\n",
    "    )\n",
    "    return masks.cpu()\n",
    "\n",
    "def draw_mask(mask, image, random_color=True):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.8])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "\n",
    "    annotated_frame_pil = Image.fromarray(image).convert(\"RGBA\")\n",
    "    mask_image_pil = Image.fromarray((mask_image.cpu().numpy() * 255).astype(np.uint8)).convert(\"RGBA\")\n",
    "    return np.array(Image.alpha_composite(annotated_frame_pil, mask_image_pil))\n",
    "\n",
    "def get_camera_pose(angle):\n",
    "    rad = np.deg2rad(angle)\n",
    "    cam_pos = np.array([np.sin(rad), 0, np.cos(rad)])  # circular orbit\n",
    "    look_at = np.array([0, 0, 0])\n",
    "    forward = look_at - cam_pos\n",
    "    forward /= np.linalg.norm(forward)\n",
    "    right = np.cross(np.array([0, 1, 0]), forward)\n",
    "    up = np.cross(forward, right)\n",
    "    rot = np.stack([right, up, forward], axis=1)\n",
    "    return rot, cam_pos\n",
    "\n",
    "def backproject(mask, cam_rot, cam_pos, voxel_grid):\n",
    "    indices = torch.nonzero(mask[0], as_tuple=False)\n",
    "    for y, x in indices:\n",
    "        direction = get_ray_direction(x.item(), y.item(), cam_rot)\n",
    "        for depth in torch.linspace(0.5, 2.0, steps=64):\n",
    "            pt = cam_pos + depth.item() * direction\n",
    "            grid_pt = torch.from_numpy((pt + 1.0) * (VOXEL_GRID_SIZE / 2)).long()\n",
    "            if ((grid_pt >= 0) & (grid_pt < VOXEL_GRID_SIZE)).all():\n",
    "                voxel_grid[grid_pt[0], grid_pt[1], grid_pt[2]] = 1\n",
    "\n",
    "def get_ray_direction(x, y, cam_rot):\n",
    "    ndc_x = (x / IMAGE_SIZE - 0.5) * 2 * np.tan(np.deg2rad(FOV/2))\n",
    "    ndc_y = (y / IMAGE_SIZE - 0.5) * 2 * np.tan(np.deg2rad(FOV/2))\n",
    "    ray_cam = np.array([ndc_x, -ndc_y, -1.0])\n",
    "    ray_cam /= np.linalg.norm(ray_cam)\n",
    "    ray_world = cam_rot @ ray_cam\n",
    "    return ray_world\n",
    "\n",
    "def segment_and_fuse():\n",
    "    view_files = sorted([f for f in os.listdir(VIEWS_DIR) if f.endswith(('.png', '.jpg'))])\n",
    "    if not view_files:\n",
    "        print(f\"No images found in {VIEWS_DIR}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(view_files)} views to process\")\n",
    "\n",
    "    for i, view_file in enumerate(view_files):\n",
    "        print(f\"Processing {view_file}...\")\n",
    "        view_path = os.path.join(VIEWS_DIR, view_file)\n",
    "        image_source, image = load_image(view_path)\n",
    "        image_source_rgb = cv2.cvtColor(image_source, cv2.COLOR_BGR2RGB)\n",
    "        image_pil = Image.fromarray(image_source_rgb)\n",
    "\n",
    "        boxes, logits, _ = predict(\n",
    "            model=groundingdino_model,\n",
    "            image=image,\n",
    "            caption=None,\n",
    "            box_threshold=BOX_THRESHOLD,\n",
    "            text_threshold=TEXT_THRESHOLD,\n",
    "            device=DEVICE\n",
    "        )\n",
    "\n",
    "        if len(boxes) == 0:\n",
    "            print(f\"No objects detected in {view_file}\")\n",
    "            continue\n",
    "\n",
    "        base_name = os.path.splitext(view_file)[0]\n",
    "        annotated = annotate(\n",
    "            image_source=image_source,\n",
    "            boxes=boxes,\n",
    "            logits=logits,\n",
    "            phrases=[TEXT_PROMPT]*len(boxes)\n",
    "        )\n",
    "\n",
    "        segmented_masks = segment(image_source, sam_predictor, boxes=boxes)\n",
    "        annotated_frame_with_mask = draw_mask(segmented_masks[0][0], annotated)\n",
    "        cv2.imwrite(os.path.join(OUTPUT_DIR, f\"{base_name}_mask.png\"), annotated_frame_with_mask)\n",
    "        \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    segment_and_fuse()\n",
    "    print(\"Segmentation and volumetric fusion complete!\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eae3788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3638.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n",
      "Found 5 views to process\n",
      "\n",
      "Processing view_0.png...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "SupervisionWarnings: annotate is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed view_0.png\n",
      "\n",
      "Processing view_1.png...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SupervisionWarnings: annotate is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed view_1.png\n",
      "\n",
      "Processing view_2.png...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SupervisionWarnings: annotate is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed view_2.png\n",
      "\n",
      "Processing view_3.png...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SupervisionWarnings: annotate is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed view_3.png\n",
      "\n",
      "Processing view_4.png...\n",
      "Successfully processed view_4.png\n",
      "Segmentation complete! Check the output directory for results.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SupervisionWarnings: annotate is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n"
     ]
    }
   ],
   "source": [
    "groundingdino_model = load_model(CONFIG_PATH, CHECKPOINT_PATH).to(DEVICE)\n",
    "sam = sam_model_registry[\"vit_h\"](checkpoint=SAM_CHECKPOINT).to(DEVICE)\n",
    "sam_predictor = SamPredictor(sam)\n",
    "\n",
    "def box_to_pixel(box, image_shape):\n",
    "    h, w = image_shape[:2]\n",
    "    return np.array([\n",
    "        int(box[0] * w),\n",
    "        int(box[1] * h),\n",
    "        int(box[2] * w),\n",
    "        int(box[3] * h)\n",
    "    ])\n",
    "\n",
    "def segment_and_save_views():\n",
    "    view_files = sorted([f for f in os.listdir(VIEWS_DIR) if f.endswith(('.png', '.jpg'))])\n",
    "    \n",
    "    if not view_files:\n",
    "        print(f\"No images found in {VIEWS_DIR}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(view_files)} views to process\")\n",
    "\n",
    "    for view_file in view_files:\n",
    "        print(f\"\\nProcessing {view_file}...\")\n",
    "        view_path = os.path.join(VIEWS_DIR, view_file)\n",
    "\n",
    "        try:\n",
    "            # Load and prepare image\n",
    "            image_source, image = load_image(view_path)\n",
    "            image_rgb = cv2.cvtColor(image_source, cv2.COLOR_BGR2RGB)\n",
    "            h, w = image_source.shape[:2]\n",
    "\n",
    "            # Get boxes from GroundingDINO\n",
    "            boxes, logits, _ = predict(\n",
    "                model=groundingdino_model,\n",
    "                image=image,\n",
    "                caption=TEXT_PROMPT,\n",
    "                box_threshold=BOX_THRESHOLD,\n",
    "                text_threshold=TEXT_THRESHOLD,\n",
    "                device=DEVICE\n",
    "            )\n",
    "\n",
    "            if len(boxes) == 0:\n",
    "                print(f\"No objects detected in {view_file}\")\n",
    "                continue\n",
    "\n",
    "            # Only use the first detected box\n",
    "            box = boxes[0].cpu().numpy()\n",
    "            box_pixel = box_to_pixel(box, image_source.shape)\n",
    "\n",
    "            # Segment using box-prompted SAM\n",
    "            sam_predictor.set_image(image_rgb)\n",
    "            masks, scores, _ = sam_predictor.predict(\n",
    "                box=box_pixel,\n",
    "                multimask_output=True\n",
    "            )\n",
    "\n",
    "            best_mask = masks[np.argmax(scores)]\n",
    "\n",
    "            # Convert to uint8 for OpenCV use\n",
    "            mask_uint8 = best_mask.astype(np.uint8) * 255\n",
    "\n",
    "            # Apply visual highlighting\n",
    "            overlay = image_source.copy()\n",
    "            overlay[best_mask > 0] = [0, 255, 0]\n",
    "            highlighted = cv2.addWeighted(overlay, 0.5, image_source, 0.5, 0)\n",
    "\n",
    "            # Draw the bounding box\n",
    "            cv2.rectangle(highlighted,\n",
    "                          (box_pixel[0], box_pixel[1]),\n",
    "                          (box_pixel[2], box_pixel[3]),\n",
    "                          (0, 0, 255), 2)\n",
    "\n",
    "            # Save files\n",
    "            base_name = os.path.splitext(view_file)[0]\n",
    "            cv2.imwrite(os.path.join(OUTPUT_DIR, f\"{base_name}_mask.png\"), mask_uint8)\n",
    "            cv2.imwrite(os.path.join(OUTPUT_DIR, f\"{base_name}_highlighted.png\"), highlighted)\n",
    "\n",
    "            # Annotated version (from GroundingDINO)\n",
    "            annotated = annotate(\n",
    "                image_source=image_source,\n",
    "                boxes=boxes,\n",
    "                logits=logits,\n",
    "                phrases=[TEXT_PROMPT] * len(boxes)\n",
    "            )\n",
    "            cv2.imwrite(os.path.join(OUTPUT_DIR, f\"{base_name}_annotated.png\"), annotated)\n",
    "\n",
    "            print(f\"Successfully processed {view_file}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {view_file}: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    segment_and_save_views()\n",
    "    print(\"Segmentation complete! Check the output directory for results.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae32a99",
   "metadata": {},
   "source": [
    "## llm integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e494d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n",
      "final text_encoder_type: bert-base-uncased\n",
      "Loading LLaVA model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards:   0%|          | 0/3 [01:39<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading LLaVA model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m llava_processor \u001b[38;5;241m=\u001b[39m AutoProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(LLAVA_MODEL_NAME)\n\u001b[0;32m---> 34\u001b[0m llava_model \u001b[38;5;241m=\u001b[39m \u001b[43mLlavaForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mLLAVA_MODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     38\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPartSegmenter\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "File \u001b[0;32m~/Desktop/llm_data_vis/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:3944\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3941\u001b[0m \u001b[38;5;66;03m# We'll need to download and cache each checkpoint shard if the checkpoint is sharded.\u001b[39;00m\n\u001b[1;32m   3942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sharded:\n\u001b[1;32m   3943\u001b[0m     \u001b[38;5;66;03m# resolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.\u001b[39;00m\n\u001b[0;32m-> 3944\u001b[0m     resolved_archive_file, sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_checkpoint_shard_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3950\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3951\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3953\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3954\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3955\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3956\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3957\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3960\u001b[0m     is_safetensors_available()\n\u001b[1;32m   3961\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resolved_archive_file, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m   3962\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m resolved_archive_file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3963\u001b[0m ):\n\u001b[1;32m   3964\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m safe_open(resolved_archive_file, framework\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/Desktop/llm_data_vis/.venv/lib/python3.11/site-packages/transformers/utils/hub.py:1098\u001b[0m, in \u001b[0;36mget_checkpoint_shard_files\u001b[0;34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m shard_filename \u001b[38;5;129;01min\u001b[39;00m tqdm(shard_filenames, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading shards\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n\u001b[1;32m   1096\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1097\u001b[0m         \u001b[38;5;66;03m# Load from URL\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m         cached_filename \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshard_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1112\u001b[0m     \u001b[38;5;66;03m# We have already dealt with RepositoryNotFoundError and RevisionNotFoundError when getting the index, so\u001b[39;00m\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;66;03m# we don't have to catch them here.\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n",
      "File \u001b[0;32m~/Desktop/llm_data_vis/.venv/lib/python3.11/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    418\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[0;32m~/Desktop/llm_data_vis/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/llm_data_vis/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:860\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m    841\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m    842\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    857\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    858\u001b[0m     )\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/llm_data_vis/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1009\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1007\u001b[0m Path(lock_path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[0;32m-> 1009\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1019\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[1;32m   1020\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/llm_data_vis/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1543\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[1;32m   1540\u001b[0m         _check_disk_space(expected_size, incomplete_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[1;32m   1541\u001b[0m         _check_disk_space(expected_size, destination_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[0;32m-> 1543\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1552\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1553\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[0;32m~/Desktop/llm_data_vis/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:452\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    450\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 452\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstants\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDOWNLOAD_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# filter out keep-alive new chunks\u001b[39;49;00m\n\u001b[1;32m    454\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/llm_data_vis/.venv/lib/python3.11/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/Desktop/llm_data_vis/.venv/lib/python3.11/site-packages/urllib3/response.py:1066\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1066\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1068\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m   1069\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/Desktop/llm_data_vis/.venv/lib/python3.11/site-packages/urllib3/response.py:955\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    952\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    953\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 955\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    957\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/llm_data_vis/.venv/lib/python3.11/site-packages/urllib3/response.py:879\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    876\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 879\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    888\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    889\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Desktop/llm_data_vis/.venv/lib/python3.11/site-packages/urllib3/response.py:862\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:473\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    472\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 473\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1314\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1311\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1312\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1313\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1166\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1168\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from GroundingDINO.groundingdino.util import box_ops\n",
    "from GroundingDINO.groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "from segment_anything.segment_anything import sam_model_registry, SamPredictor\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "from typing import List, Optional\n",
    "\n",
    "# Configuration\n",
    "CONFIG_PATH = \"./GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
    "CHECKPOINT_PATH = \"./groundingdino_swint_ogc.pth\"\n",
    "SAM_CHECKPOINT = \"../models/sam_vit_h_4b8939.pth\"\n",
    "LLAVA_MODEL_NAME = \"llava-hf/llava-1.5-7b-hf\"  # Open-source VLM\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "TEXT_PROMPT = \"tail\"  # Default prompt\n",
    "BOX_THRESHOLD = 0.3\n",
    "TEXT_THRESHOLD = 0.25\n",
    "VIEWS_DIR = \"../view/\"\n",
    "OUTPUT_DIR = \"../segmented_views/\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize models\n",
    "print(\"Loading models...\")\n",
    "groundingdino_model = load_model(CONFIG_PATH, CHECKPOINT_PATH).to(DEVICE)\n",
    "sam = sam_model_registry[\"vit_h\"](checkpoint=SAM_CHECKPOINT).to(DEVICE)\n",
    "sam_predictor = SamPredictor(sam)\n",
    "\n",
    "# Initialize LLaVA (vision-language model)\n",
    "print(\"Loading LLaVA model...\")\n",
    "llava_processor = AutoProcessor.from_pretrained(LLAVA_MODEL_NAME)\n",
    "llava_model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    LLAVA_MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
    "    low_cpu_mem_usage=True\n",
    ").to(DEVICE)\n",
    "\n",
    "class PartSegmenter:\n",
    "    def __init__(self):\n",
    "        self.device = DEVICE\n",
    "        \n",
    "    def refine_prompt_with_llm(self, image: np.ndarray, initial_prompt: str) -> str:\n",
    "        \"\"\"Use LLaVA to refine the text prompt based on image content.\"\"\"\n",
    "        try:\n",
    "            # Convert numpy array to PIL Image\n",
    "            image_pil = Image.fromarray(image)\n",
    "            \n",
    "            # Prepare prompt for LLaVA - using the correct format\n",
    "            prompt = f\"USER: <image>\\nWhat is the most precise way to describe the {initial_prompt} in this image for segmentation purposes? Just provide the description without additional text.\\nASSISTANT:\"\n",
    "            \n",
    "            # Process inputs with correct text format\n",
    "            inputs = llava_processor(\n",
    "                text=prompt,  # Single string input\n",
    "                images=image_pil,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True\n",
    "            ).to(DEVICE, torch.float16)\n",
    "            \n",
    "            # Generate response\n",
    "            generate_ids = llava_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=50,\n",
    "                do_sample=True\n",
    "            )\n",
    "            \n",
    "            # Decode response\n",
    "            response = llava_processor.decode(\n",
    "                generate_ids[0],\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=False\n",
    "            )\n",
    "            \n",
    "            # Extract just the assistant's response\n",
    "            if \"ASSISTANT:\" in response:\n",
    "                refined_prompt = response.split(\"ASSISTANT:\")[-1].strip()\n",
    "            else:\n",
    "                refined_prompt = initial_prompt\n",
    "                \n",
    "            # Fallback if response is not useful\n",
    "            if len(refined_prompt) < 3 or initial_prompt.lower() not in refined_prompt.lower():\n",
    "                return initial_prompt\n",
    "                \n",
    "            return refined_prompt\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"LLaVA prompt refinement failed: {str(e)}\")\n",
    "            return initial_prompt\n",
    "    \n",
    "    def validate_mask_with_llm(self, image: np.ndarray, mask: np.ndarray, prompt: str) -> bool:\n",
    "        \"\"\"Use LLaVA to validate if the masked region matches the prompt.\"\"\"\n",
    "        try:\n",
    "            # Create masked image (ensure it's the right type)\n",
    "            masked_image = (image * (mask[:, :, np.newaxis] if mask.ndim == 2 else mask)).astype(np.uint8)\n",
    "            masked_image_pil = Image.fromarray(masked_image)\n",
    "            \n",
    "            # Prepare prompt for LLaVA - single string format\n",
    "            validation_prompt = (\n",
    "                f\"USER: <image>\\nDoes this image show a {prompt}? \"\n",
    "                f\"Answer with just 'yes' or 'no'.\\nASSISTANT:\"\n",
    "            )\n",
    "            \n",
    "            # Process inputs with correct text format\n",
    "            inputs = llava_processor(\n",
    "                text=validation_prompt,  # Single string input\n",
    "                images=masked_image_pil,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True\n",
    "            ).to(DEVICE, torch.float16)\n",
    "            \n",
    "            # Generate response\n",
    "            generate_ids = llava_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=2,  # We just want yes/no\n",
    "                do_sample=False  # Disable sampling for deterministic output\n",
    "            )\n",
    "            \n",
    "            # Decode response\n",
    "            response = llava_processor.decode(\n",
    "                generate_ids[0],\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=False\n",
    "            )\n",
    "            \n",
    "            # Check for affirmative response\n",
    "            return \"yes\" in response.lower()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"LLaVA validation failed: {str(e)}\")\n",
    "            return True  # Default to keeping mask if validation fails\n",
    "\n",
    "def segment(image: np.ndarray, sam_model: SamPredictor, boxes: torch.Tensor) -> List[np.ndarray]:\n",
    "    \"\"\"Enhanced segmentation with multi-mask output\"\"\"\n",
    "    sam_model.set_image(image)\n",
    "    H, W, _ = image.shape\n",
    "    boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H])\n",
    "\n",
    "    transformed_boxes = sam_model.transform.apply_boxes_torch(boxes_xyxy.to(DEVICE), image.shape[:2])\n",
    "    \n",
    "    # Get multiple masks per box\n",
    "    masks, _, _ = sam_model.predict_torch(\n",
    "        point_coords=None,\n",
    "        point_labels=None,\n",
    "        boxes=transformed_boxes,\n",
    "        multimask_output=True,\n",
    "    )\n",
    "    \n",
    "    # Convert to numpy and filter empty masks\n",
    "    all_masks = []\n",
    "    for box_masks in masks:\n",
    "        for mask in box_masks:\n",
    "            mask_np = mask.cpu().numpy()\n",
    "            if mask_np.sum() > 100:  # Minimum pixel threshold\n",
    "                all_masks.append(mask_np)\n",
    "    \n",
    "    return all_masks\n",
    "\n",
    "def draw_mask(mask: np.ndarray, image: np.ndarray, random_color: bool = True) -> np.ndarray:\n",
    "    \"\"\"Draw mask on image with optional random color\"\"\"\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.8])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    \n",
    "    h, w = mask.shape\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    \n",
    "    annotated_frame_pil = Image.fromarray(image).convert(\"RGBA\")\n",
    "    mask_image_pil = Image.fromarray((mask_image * 255).astype(np.uint8)).convert(\"RGBA\")\n",
    "\n",
    "    return np.array(Image.alpha_composite(annotated_frame_pil, mask_image_pil))\n",
    "\n",
    "def segment_and_save_views():\n",
    "    \"\"\"Enhanced segmentation pipeline with LLM integration\"\"\"\n",
    "    view_files = sorted([f for f in os.listdir(VIEWS_DIR) if f.endswith(('.png', '.jpg'))])\n",
    "    \n",
    "    if not view_files:\n",
    "        print(f\"No images found in {VIEWS_DIR}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(view_files)} views to process\")\n",
    "    segmenter = PartSegmenter()\n",
    "    \n",
    "    for view_file in view_files:\n",
    "        print(f\"Processing {view_file}...\")\n",
    "        view_path = os.path.join(VIEWS_DIR, view_file)\n",
    "        \n",
    "        try:\n",
    "            # Load and prepare image\n",
    "            image_source, image = load_image(view_path)\n",
    "            image_source_rgb = cv2.cvtColor(image_source, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Refine prompt using LLaVA\n",
    "            refined_prompt = segmenter.refine_prompt_with_llm(image_source_rgb, TEXT_PROMPT)\n",
    "            print(f\"Original prompt: {TEXT_PROMPT}, Refined prompt: {refined_prompt}\")\n",
    "            \n",
    "            # Get boxes from GroundingDINO\n",
    "            boxes, logits, _ = predict(\n",
    "                model=groundingdino_model,\n",
    "                image=image,\n",
    "                caption=refined_prompt,\n",
    "                box_threshold=BOX_THRESHOLD,\n",
    "                text_threshold=TEXT_THRESHOLD,\n",
    "                device=DEVICE\n",
    "            )\n",
    "            \n",
    "            if len(boxes) == 0:\n",
    "                print(f\"No objects detected in {view_file}\")\n",
    "                continue\n",
    "            \n",
    "            # Get masks\n",
    "            masks = segment(image_source_rgb, sam_predictor, boxes=boxes)\n",
    "            \n",
    "            # Validate masks with LLaVA\n",
    "            valid_masks = []\n",
    "            for mask in masks:\n",
    "                if segmenter.validate_mask_with_llm(image_source_rgb, mask, refined_prompt):\n",
    "                    valid_masks.append(mask)\n",
    "                else:\n",
    "                    print(\"LLaVA rejected a mask as not matching the prompt\")\n",
    "            \n",
    "            if not valid_masks:\n",
    "                print(f\"No valid masks found for {view_file}\")\n",
    "                continue\n",
    "            \n",
    "            # Save results\n",
    "            base_name = os.path.splitext(view_file)[0]\n",
    "            \n",
    "            # Save annotation with boxes\n",
    "            annotated = annotate(\n",
    "                image_source=image_source,\n",
    "                boxes=boxes,\n",
    "                logits=logits,\n",
    "                phrases=[refined_prompt]*len(boxes)\n",
    "            )\n",
    "            \n",
    "            # Save each valid mask\n",
    "            for i, mask in enumerate(valid_masks):\n",
    "                annotated_frame_with_mask = draw_mask(mask, annotated)\n",
    "                cv2.imwrite(\n",
    "                    os.path.join(OUTPUT_DIR, f\"{base_name}_mask_{i}.png\"),\n",
    "                    cv2.cvtColor(annotated_frame_with_mask, cv2.COLOR_RGBA2BGR)\n",
    "                )\n",
    "            \n",
    "            cv2.imwrite(os.path.join(OUTPUT_DIR, f\"{base_name}_annotated.png\"), annotated)\n",
    "            print(f\"Saved results for {view_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {view_file}: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    segment_and_save_views()\n",
    "    print(\"Segmentation complete! Check the output directory for results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29447ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from GroundingDINO.groundingdino.util.inference import load_model as load_groundingdino, predict\n",
    "from segment_anything.segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "# Load Grounding DINO\n",
    "grounding_dino = load_groundingdino(\n",
    "    \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\",\n",
    "    \"groundingdino_swint_ogc.pth\"\n",
    ")\n",
    "\n",
    "# Load SAM\n",
    "sam = sam_model_registry[\"vit_h\"](checkpoint=\"sam_vit_h_4b8939.pth\")\n",
    "sam_predictor = SamPredictor(sam)\n",
    "\n",
    "# Load Open-source LLM (e.g., LLaMA-3 via HuggingFace)\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "llm = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6f2989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_part_prompts(object_description):\n",
    "    prompt = f\"\"\"\n",
    "    You are a vision assistant. List unambiguous text prompts to segment all parts of: {object_description}.\n",
    "    Use spatial references (e.g., 'left ear', 'central loop of the knot'). Return as a Python list.\n",
    "    Example output for 'elephant':\n",
    "    ['elephant trunk', 'left front leg', 'right ear', 'tail']\n",
    "    \"\"\"\n",
    "    inputs = llm_tokenizer(prompt, return_tensors=\"pt\").to(\"cpu\")\n",
    "    outputs = llm.generate(**inputs, max_new_tokens=100)\n",
    "    return eval(llm_tokenizer.decode(outputs[0], skip_special_tokens=True).split(\"```\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeb7ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_parts(image_path, object_class):\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Step 1: Detect whole object\n",
    "    boxes, _, _ = predict(\n",
    "        grounding_dino, image_rgb, caption=object_class, box_threshold=0.3\n",
    "    )\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "\n",
    "    # Step 2: Generate part prompts with LLM\n",
    "    part_prompts = generate_part_prompts(object_class)\n",
    "\n",
    "    # Step 3: Detect and segment parts\n",
    "    sam_predictor.set_image(image_rgb)\n",
    "    all_masks = []\n",
    "    for prompt in part_prompts:\n",
    "        part_boxes, _, _ = predict(\n",
    "            grounding_dino, image_rgb, caption=prompt, box_threshold=0.2\n",
    "        )\n",
    "        for box in part_boxes:\n",
    "            # Convert box to SAM input (normalized XYXY)\n",
    "            box_xyxy = box / np.array([image.shape[1], image.shape[0]] * 2)\n",
    "            masks, _, _ = sam_predictor.predict(\n",
    "                box=box_xyxy[None, :],\n",
    "                multimask_output=True  # Get 3 candidate masks\n",
    "            )\n",
    "            all_masks.append(masks[0])  # Select the best mask\n",
    "\n",
    "    return all_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7afd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_masks(masks, iou_threshold=0.5):\n",
    "    # Non-Max Suppression (NMS) to remove duplicates\n",
    "    from torchvision.ops import nms\n",
    "    boxes = torch.tensor([m.sum(axis=(0, 1)) for m in masks])  # Dummy boxes\n",
    "    scores = torch.ones(len(masks))  # Dummy scores\n",
    "    keep = nms(boxes, scores, iou_threshold)\n",
    "    return [masks[i] for i in keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1a302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Segment parts of an elephant\n",
    "masks = segment_parts(\"objects/1a6f615e8b1b5ae4dbbc9440457e303e/rendered_data/0_softflat_gray.png\", \"chair\")\n",
    "masks = clean_masks(masks)\n",
    "\n",
    "# Visualize\n",
    "for mask in masks:\n",
    "    cv2.imshow(\"Part\", mask.astype(np.uint8) * 255)\n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef21c939",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
